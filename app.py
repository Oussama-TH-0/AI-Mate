import os
import gradio as gr
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
import logging
import tempfile
import traceback

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

GOOGLE_API_KEY = os.getenv("gem_api")

try:
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        temperature=0.2,
        google_api_key=GOOGLE_API_KEY,
        max_tokens=2048,
        timeout=60
    )
except Exception as e:
    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
    raise

try:
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
        cache_folder="./embeddings_cache"
    )
except Exception as e:
    logger.warning(f"ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ BAAI/bge-m3ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: {e}")
    embeddings = HuggingFaceEmbeddings()

PROMPT_TEMPLATE = """
if the question is in english anwser in english 
Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ø¬Ø¨ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
Ø§Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…Ø®ØµØµ Ù„Ù„Ø§Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª ÙˆÙ…ÙˆØ¬Ù‡ Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø­Ù„Ø¨ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©. 
Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ù„ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.
Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
- Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„: "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©"
- Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ù„ØºØ© ÙˆÙ„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ 
- Ø§ÙƒØªØ¨ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø¨Ø´ÙƒÙ„ Ù†ØµÙŠ ÙˆØ§Ø¶Ø­ (ØªØ¬Ù†Ø¨ LaTeX)
- Ø§Ø°Ø§ Ø³Ø£Ù„Ùƒ Ø¹Ù† Ø´Ø±Ø­ Ø§Ø¶Ø§ÙÙŠ Ø§Ø´Ø±Ø­ Ø§Ùˆ Ø§Ø¶Ù Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙƒ ÙˆÙ„ÙƒÙ† Ù„Ø§ ØªØ®Ø±Ø¬ Ø¹Ù† Ø³ÙŠØ§Ù‚ ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯
- Ø±Ø§Ø¹Ù Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ­Ø§ÙˆØ±ÙŠ ÙˆØ§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ´ÙŠØ± Ù„Ø´ÙŠØ¡ Ø³Ø§Ø¨Ù‚ (Ù…Ø«Ù„ "Ø§Ø´Ø±Ø­ Ø£ÙƒØ«Ø±" Ø£Ùˆ "ÙˆØ¶Ù‘Ø­ Ù‡Ø°Ø§")ØŒ Ø§Ø±Ø¬Ø¹ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
- Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø´ÙŠØ¦ Ù…ÙˆØ¬ÙˆØ¯ ÙƒÙ…Ø¹Ù„ÙˆÙ…Ø© ÙˆÙ„ÙƒÙ† Ù„Ù… ÙŠØ°ÙƒØ± Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ Ø§Ø¬Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙˆÙ…Ø¹Ø±ÙØªÙƒ
- Ø§Ø°Ø§ Ø§Ø±Ø¯Øª Ø·Ø±Ø­ Ù…Ø«Ø§Ù„ Ø§Ùˆ Ø·Ù„Ø¨ Ù…Ù†Ùƒ Ø³Ø¤Ø§Ù„ Ø§Ùˆ Ù…Ø«Ø§Ù„ Ø¹Ù† ÙÙƒØ±Ø© Ø§Ø¬Ø¨ Ø¨Ù…Ø«Ø§Ù„ Ø§Ùˆ Ø³Ø¤Ø§Ù„ Ù…Ù† Ù…Ø¹Ø±ÙØªÙƒ ÙˆÙ„ÙƒÙ† Ø¨Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø§Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ù…Ù„Ù 
**Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:**
{context}
**Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:**
{chat_history}
**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ:**
{question}
**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**
"""

prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "chat_history", "question"])

retriever = None
vector_store = None
current_file_name = ""
conversation_history = [] 

def upload_file(file):
    
    global retriever, vector_store, current_file_name
    
    if file is None:
        return "âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø®ØªÙŠØ§Ø± Ù…Ù„Ù PDF"
    
    try:
        if not file.name.lower().endswith('.pdf'):
            return "âŒ ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„Ù PDF ÙÙ‚Ø·"
                
        loader = PyMuPDFLoader(file.name)
        documents = loader.load()
        
        if not documents:
            return "âŒ Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡ØªÙ‡"
                
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,  
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        docs_chunks = splitter.split_documents(documents)
        
        if not docs_chunks:
            return "âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ†Ø¯"
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(docs_chunks)} Ù‚Ø·Ø¹Ø© Ù†ØµÙŠØ©")
        
        vector_store = FAISS.from_documents(docs_chunks, embeddings)
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={'k': 5, 'fetch_k': 10}  
        )
        
        current_file_name = os.path.basename(file.name)
        
        return f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù '{current_file_name}' Ø¨Ù†Ø¬Ø§Ø­! ({len(docs_chunks)} Ù‚Ø·Ø¹Ø© Ù†ØµÙŠØ©)"
        
    except Exception as e:
        return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}"

def format_chat_history(history):
    
    if not history:
        return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø©."
    
    formatted_history = []
    
    recent_history = history[-3:] if len(history) > 3 else history
    
    for i, (q, a) in enumerate(recent_history):
        formatted_history.append(f"Ø§Ù„Ø³Ø¤Ø§Ù„ {i+1}: {q}")
        formatted_history.append(f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© {i+1}: {a}")
    
    return "\n".join(formatted_history)

def detect_reference_question(question, history):
    reference_keywords = [
        'Ø§Ø´Ø±Ø­ Ø£ÙƒØ«Ø±', 'ÙˆØ¶Ø­ Ø£ÙƒØ«Ø±', 'Ø£Ø¹Ø·Ù†ÙŠ ØªÙØ§ØµÙŠÙ„', 'Ø²ÙŠØ¯ Ø¹Ù„ÙŠÙ‡Ø§', 'ÙƒÙ…Ù‘Ù„', 'Ø£ÙƒÙ…Ù„',
        'ÙˆØ¶Ø­ Ù‡Ø°Ø§', 'Ø§Ø´Ø±Ø­ Ù‡Ø°Ø§', 'ØªÙØµÙŠÙ„', 'ØªÙØµÙŠÙ„Ø§Ù‹', 'Ø¨Ø§Ù„ØªÙØµÙŠÙ„', 'Ø£ÙˆØ³Ø¹',
        'Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹', 'Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø·Ø©', 'Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©', 'Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø³Ø§Ø¨Ù‚',
        'Ù…Ø§Ø°Ø§ Ø¹Ù†', 'ÙƒÙŠÙ Ø°Ù„Ùƒ', 'Ù„Ù…Ø§Ø°Ø§', 'ÙƒÙŠÙ ÙŠØ­Ø¯Ø«', 'Ù…Ø§ Ø§Ù„Ù…Ù‚ØµÙˆØ¯',
        'explain more', 'elaborate', 'tell me more', 'expand on', 'details',
        'clarify', 'what about', 'how so', 'why', 'what do you mean',
        'this topic', 'that point', 'previous', 'above', 'mentioned'
    ]
    
    question_lower = question.lower()
    
    if len(question.split()) <= 10:
        for keyword in reference_keywords:
            if keyword in question_lower:
                return True
    
    reference_pronouns = ['Ù‡Ø°Ø§', 'Ù‡Ø°Ù‡', 'Ø°Ù„Ùƒ', 'ØªÙ„Ùƒ', 'this', 'that', 'it', 'they']
    for pronoun in reference_pronouns:
        if pronoun in question_lower and len(question.split()) <= 15:
            return True
    
    return False

def ask_bot(question, history):
    
    global conversation_history
    
    if not question or not question.strip():
        return history, ""
    
    if not retriever:
        error_msg = "ğŸ›‘ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ Ù…Ù„Ù PDF Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"
        return history + [[question, error_msg]], ""
    
    try:        
        # ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        conversation_history = history.copy()
        
        # ØªÙƒÙˆÙŠÙ† ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        chat_history_text = format_chat_history(conversation_history)
        
        # ÙƒØ´Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø±Ø¬Ø¹ÙŠ
        is_reference = detect_reference_question(question, conversation_history)
        
        if is_reference and conversation_history:
            logger.info("ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø³Ø¤Ø§Ù„ Ù…Ø±Ø¬Ø¹ÙŠ - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ­Ø§ÙˆØ±ÙŠ")
            
            enhanced_question = f"{question}\n\nÙ…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ´ÙŠØ± Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø³ÙŠØ§Ù‚."
        else:
            enhanced_question = question
        
        relevant_docs = retriever.get_relevant_documents(enhanced_question)
        context_text = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        rag_chain = (prompt | llm | StrOutputParser())
        
        prompt_inputs = {"context": context_text,
            "chat_history": chat_history_text,
            "question": enhanced_question
        }
        
        response = rag_chain.invoke(prompt_inputs)
        
        if response:
            response = response.strip()
        else:
            response = "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©"
        
        if is_reference:
            response = f"ğŸ”— {response}"
        
        logger.info("ØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†Ø¬Ø§Ø­")
        new_history = history + [[question, response]]
        
        return new_history, ""
        
    except Exception as e:
        error_msg = f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"
        return history + [[question, error_msg]], ""

def clear_chat():
    
    global conversation_history
    conversation_history = []  
    return [], ""

def get_file_info():
    
    if current_file_name and vector_store:
        return f"ğŸ“„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ: {current_file_name}"
    return "âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù Ù…Ø­Ù…Ù„"

with gr.Blocks(
    theme=gr.themes.Soft(),
    title="AI Study Mate",
    css="""
    .gradio-container {font-family: 'Arial', sans-serif;}
    .chat-message {border-radius: 10px; padding: 10px; margin: 5px 0;}
    """
) as demo:
    
    gr.Markdown("""
    # ğŸ“ **Your AI Studying Mate**
    ### This AI is Developed By Osama Touma Halabi ğŸ‘¨â€ğŸ’» 
    """)
    
    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(label="ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©",height=500,bubble_full_width=False,show_label=True)
            
            with gr.Row():
                question = gr.Textbox(placeholder="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...",label="â“ Ø³Ø¤Ø§Ù„Ùƒ",lines=2,scale=4)
                
                send_btn = gr.Button("ğŸ“¤ Ø£Ø±Ø³Ù„", scale=1, variant="primary")
            
            with gr.Row():
                clear_btn = gr.Button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", scale=1)
                info_btn = gr.Button("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù", scale=1)
        
        with gr.Column(scale=1):
            file_upload = gr.File(label="ğŸ“‚ Ø§Ø±ÙØ¹ Ù…Ù„Ù PDF",file_types=[".pdf"],height=200)
            
            upload_status = gr.Textbox(label="ğŸ“‹ Ø­Ø§Ù„Ø© Ø§Ù„Ø±ÙØ¹",interactive=False,lines=3)
            
            gr.Markdown("""
            ### ğŸ’¡ Ù†ØµØ§Ø¦Ø­:
            - Ø§Ø±ÙØ¹ Ù…Ù„Ù PDF ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù‚Ø±ÙˆØ¡
            - Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø¨Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©
            - Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø©
            - ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            - ÙŠØªØ°ÙƒØ± Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©! ÙŠÙ…ÙƒÙ†Ùƒ Ù‚ÙˆÙ„ "Ø§Ø´Ø±Ø­ Ø£ÙƒØ«Ø±" Ø£Ùˆ "ÙˆØ¶Ù‘Ø­ Ù‡Ø°Ø§ 
            - ÙŠØ­ÙØ¸ Ø¢Ø®Ø± 3 Ø£Ø³Ø¦Ù„Ø© ÙˆØ¥Ø¬Ø§Ø¨Ø§Øª Ù„Ù„Ø³ÙŠØ§Ù‚
            - ÙÙŠ Ø­Ø§Ù„ Ø®Ø±Ø¬ Ø¹Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ùˆ Ù„Ù… ÙŠØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©
            Ù‚Ù… Ø¨Ø³Ù…Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ø¨Ø¯Ø§ Ù…Ù† Ø¬Ø¯ÙŠØ¯
            """)
    
    send_btn.click(fn=ask_bot,inputs=[question, chatbot],outputs=[chatbot, question])
    
    question.submit(fn=ask_bot,inputs=[question, chatbot],outputs=[chatbot, question])
    
    file_upload.change(fn=upload_file,inputs=file_upload,outputs=upload_status)
    
    clear_btn.click(fn=clear_chat,outputs=[chatbot, question])
    
    info_btn.click(fn=get_file_info,outputs=upload_status)
    

if __name__ == "__main__":
    demo.launch(
        share=True,        
    )
